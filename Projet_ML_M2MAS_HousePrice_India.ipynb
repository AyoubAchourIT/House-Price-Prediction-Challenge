{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe190188",
   "metadata": {},
   "source": [
    "# Projet final — Machine Learning (M2 MAS)  \n",
    "## House Price Prediction (India) — Kaggle\n",
    "\n",
    "**Objectif :** prédire le prix d’un bien immobilier en Inde (en *lacs* = 100 000 INR) à partir de caractéristiques (type d’annonce, surface, localisation, etc.).\n",
    "\n",
    "**Source des données :** Kaggle — *House Price Prediction Challenge*  \n",
    "Lien : https://www.kaggle.com/anmolkumar/house-price-prediction-challenge\n",
    "\n",
    "> Ce notebook est écrit pour être **reproductible** : exécuter *Kernel → Restart & Run All* doit produire les mêmes résultats (même split, mêmes CV, mêmes hyperparamètres sélectionnés).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3545ea5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, KFold, RepeatedKFold, GridSearchCV, cross_validate\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports & configuration\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH  = \"test.csv\"\n",
    "SUB_PATH   = \"sample_submission.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Chargement des données\n",
    "# =========================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SUB_PATH)\n",
    "\n",
    "print(\"train:\", train.shape)\n",
    "print(\"test :\", test.shape)\n",
    "display(train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209ba78",
   "metadata": {},
   "source": [
    "## 1. Décrire les objectifs et les données\n",
    "\n",
    "On dispose de deux fichiers principaux :\n",
    "\n",
    "- **train.csv** : données d'entraînement **avec** la cible `TARGET(PRICE_IN_LACS)`  \n",
    "- **test.csv** : données de test **sans** la cible (à prédire)\n",
    "\n",
    "La variable cible est un **prix en lacs**.\n",
    "\n",
    "Dans la suite, on va :\n",
    "1. explorer les variables, vérifier les valeurs manquantes, les distributions, et quelques relations simples ;\n",
    "2. construire une pipeline de prétraitement (imputation + encodage) ;\n",
    "3. comparer plusieurs modèles dont au moins un **modèle ensembliste** ;\n",
    "4. sélectionner des hyperparamètres avec **cross-validation** ;\n",
    "5. conclure avec une évaluation finale sur un jeu de test (split) + génération éventuelle d’un fichier de soumission Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 2) Exploration, visualisation, statistiques\n",
    "# ======================================\n",
    "target_col = \"TARGET(PRICE_IN_LACS)\"\n",
    "assert target_col in train.columns\n",
    "\n",
    "# Séparation X / y\n",
    "X = train.drop(columns=[target_col])\n",
    "y = train[target_col].copy()\n",
    "\n",
    "# Informations globales\n",
    "display(X.describe(include=\"all\").T.head(30))\n",
    "print(\"\\nValeurs manquantes (train):\")\n",
    "display(X.isna().mean().sort_values(ascending=False).head(20))\n",
    "\n",
    "print(\"\\nDistribution de la cible (y):\")\n",
    "display(y.describe())\n",
    "\n",
    "# Visualisation : distribution cible\n",
    "plt.figure()\n",
    "plt.hist(y, bins=60)\n",
    "plt.title(\"Distribution de la cible: TARGET(PRICE_IN_LACS)\")\n",
    "plt.xlabel(\"Prix (lacs)\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()\n",
    "\n",
    "# Visualisation : log(1+y) (souvent utile pour les prix)\n",
    "plt.figure()\n",
    "plt.hist(np.log1p(y), bins=60)\n",
    "plt.title(\"Distribution de log(1 + prix)\")\n",
    "plt.xlabel(\"log(1 + prix)\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()\n",
    "\n",
    "# Quelques checks de cardinalité des variables catégorielles\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Colonnes catégorielles:\", cat_cols)\n",
    "print(\"Colonnes numériques     :\", num_cols)\n",
    "\n",
    "for c in cat_cols:\n",
    "    print(f\"{c}: {X[c].nunique()} valeurs distinctes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce7e7d",
   "metadata": {},
   "source": [
    "### Petite feature engineering (simple, justifiée)\n",
    "\n",
    "La colonne **`ADDRESS`** a une cardinalité élevée (beaucoup d’adresses uniques).  \n",
    "Un encodage one-hot direct peut créer un très grand nombre de colonnes, ce qui peut ralentir l’optimisation d’hyperparamètres.\n",
    "\n",
    "Approche simple et robuste :\n",
    "- extraire une variable **`CITY`** à partir de `ADDRESS` (ce qui réduit fortement la cardinalité) ;\n",
    "- **retirer `ADDRESS`**.\n",
    "\n",
    "C’est une hypothèse raisonnable : la ville capture une grande partie de l’information géographique, sans créer des milliers de modalités.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f484362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Feature engineering: CITY depuis ADDRESS\n",
    "# ======================================\n",
    "X_fe = X.copy()\n",
    "test_fe = test.copy()\n",
    "\n",
    "def add_city(df):\n",
    "    df = df.copy()\n",
    "    if \"ADDRESS\" in df.columns:\n",
    "        df[\"CITY\"] = df[\"ADDRESS\"].astype(str).str.split(\",\").str[-1].str.strip()\n",
    "        df = df.drop(columns=[\"ADDRESS\"])\n",
    "    return df\n",
    "\n",
    "X_fe = add_city(X_fe)\n",
    "test_fe = add_city(test_fe)\n",
    "\n",
    "print(\"Après FE:\", X_fe.shape)\n",
    "print(\"Nunique CITY:\", X_fe[\"CITY\"].nunique())\n",
    "\n",
    "# Visualisation rapide: prix moyen par type d'annonce (POSTED_BY)\n",
    "tmp = pd.concat([X_fe[[\"POSTED_BY\"]], y], axis=1)\n",
    "by_posted = tmp.groupby(\"POSTED_BY\")[target_col].mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(by_posted.index.astype(str), by_posted.values)\n",
    "plt.title(\"Prix moyen par POSTED_BY\")\n",
    "plt.xlabel(\"POSTED_BY\")\n",
    "plt.ylabel(\"Prix moyen (lacs)\")\n",
    "plt.show()\n",
    "\n",
    "# Top villes (fréquence)\n",
    "top_cities = X_fe[\"CITY\"].value_counts().head(15)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(top_cities.index, top_cities.values)\n",
    "plt.title(\"Top 15 villes (fréquence dans train)\")\n",
    "plt.xticks(rotation=60, ha=\"right\")\n",
    "plt.ylabel(\"Nombre d'observations\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054e515",
   "metadata": {},
   "source": [
    "## 3. Scinder le jeu de données (train/test)\n",
    "\n",
    "On réserve une partie des données pour l’évaluation finale, afin d’estimer la performance sur des données non vues.  \n",
    "On utilise un split aléatoire **reproductible** (random_state fixé).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_fe, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110336c6",
   "metadata": {},
   "source": [
    "## 4. Modèles & recherche d’hyperparamètres (Cross-validation)\n",
    "\n",
    "Exigence : **au moins 2 modèles**, dont **au moins 1 ensembliste**.\n",
    "\n",
    "On va comparer :\n",
    "- **Ridge** (baseline de régression linéaire régularisée — utile pour benchmark, même si non compté comme “modèle principal”),\n",
    "- **RandomForestRegressor** (ensembliste par bagging),\n",
    "- **HistGradientBoostingRegressor** (ensembliste par boosting, performant sur données tabulaires).\n",
    "\n",
    "### Prétraitement (pipeline)\n",
    "- colonnes numériques : imputation médiane\n",
    "- colonnes catégorielles : imputation de la modalité la plus fréquente + OneHotEncoder\n",
    "\n",
    "### Cible (prix)\n",
    "Les prix sont souvent très asymétriques → on teste un apprentissage sur **log(1 + y)** via `TransformedTargetRegressor`, puis on revient automatiquement à l’échelle “prix” pour l’évaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Prétraitement commun\n",
    "# ======================================\n",
    "cat_cols = X_fe.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = [c for c in X_fe.columns if c not in cat_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def evaluate_regression(y_true, y_pred, title=\"\"):\n",
    "    print(title)\n",
    "    print(\"MAE :\", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"RMSE:\", rmse(y_true, y_pred))\n",
    "    print(\"R2  :\", r2_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81851a20",
   "metadata": {},
   "source": [
    "### Choix du cross-validateur\n",
    "\n",
    "On illustre deux validateurs :\n",
    "\n",
    "- **KFold** (5 folds) : standard, simple et rapide  \n",
    "- **RepeatedKFold** : plus robuste (répète plusieurs fois le KFold), utile quand on veut réduire la variance de l’estimation\n",
    "\n",
    "Dans la sélection d’hyperparamètres, on utilisera surtout **KFold** (pour maîtriser le temps de calcul) puis on pourra **vérifier** le meilleur modèle avec RepeatedKFold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_kfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_rep   = RepeatedKFold(n_splits=5, n_repeats=2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Scorings (on veut minimiser l'erreur -> on utilisera neg RMSE / neg MAE)\n",
    "scoring = {\n",
    "    \"neg_rmse\": \"neg_root_mean_squared_error\",\n",
    "    \"neg_mae\": \"neg_mean_absolute_error\",\n",
    "    \"r2\": \"r2\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d11ff",
   "metadata": {},
   "source": [
    "### Grilles d’hyperparamètres\n",
    "\n",
    "Les grilles ci-dessous sont volontairement **raisonnables** (temps de calcul).  \n",
    "Vous pouvez les élargir si vous avez du temps CPU.\n",
    "\n",
    "> Astuce : sur Kaggle, le split train/valid interne n’est pas visible ; ici, on suit la consigne M2 : **CV sur train**, puis **évaluation finale** sur un hold-out (valid).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85910f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 4.a) Ridge (baseline)\n",
    "# ======================================\n",
    "ridge = Ridge(random_state=RANDOM_STATE)\n",
    "\n",
    "ridge_model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"regressor\", TransformedTargetRegressor(\n",
    "        regressor=ridge,\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1\n",
    "    ))\n",
    "])\n",
    "\n",
    "ridge_grid = {\n",
    "    \"regressor__regressor__alpha\": [0.1, 1.0, 10.0, 50.0],\n",
    "}\n",
    "\n",
    "# ======================================\n",
    "# 4.b) RandomForest (ensembliste)\n",
    "# ======================================\n",
    "rf = RandomForestRegressor(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"regressor\", TransformedTargetRegressor(\n",
    "        regressor=rf,\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_grid = {\n",
    "    \"regressor__regressor__n_estimators\": [300, 600],\n",
    "    \"regressor__regressor__max_depth\": [None, 10, 20],\n",
    "    \"regressor__regressor__min_samples_split\": [2, 10],\n",
    "    \"regressor__regressor__min_samples_leaf\": [1, 5],\n",
    "}\n",
    "\n",
    "# ======================================\n",
    "# 4.c) HistGradientBoosting (ensembliste)\n",
    "# ======================================\n",
    "hgb = HistGradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "hgb_model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"regressor\", TransformedTargetRegressor(\n",
    "        regressor=hgb,\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1\n",
    "    ))\n",
    "])\n",
    "\n",
    "hgb_grid = {\n",
    "    \"regressor__regressor__learning_rate\": [0.03, 0.06, 0.1],\n",
    "    \"regressor__regressor__max_depth\": [None, 6, 10],\n",
    "    \"regressor__regressor__max_iter\": [400, 800],\n",
    "    \"regressor__regressor__l2_regularization\": [0.0, 0.1, 1.0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247592b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 4.d) GridSearchCV (CV sur le train)\n",
    "# ======================================\n",
    "def run_gridsearch(name, model, grid, cv):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"GridSearch — {name}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=grid,\n",
    "        cv=cv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Best CV RMSE:\", -gs.best_score_)\n",
    "    print(\"Best params :\", gs.best_params_)\n",
    "    return gs\n",
    "\n",
    "gs_ridge = run_gridsearch(\"Ridge (baseline)\", ridge_model, ridge_grid, cv_kfold)\n",
    "gs_rf    = run_gridsearch(\"RandomForest\", rf_model, rf_grid, cv_kfold)\n",
    "gs_hgb   = run_gridsearch(\"HistGradientBoosting\", hgb_model, hgb_grid, cv_kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b33154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Comparaison sur le hold-out (valid)\n",
    "# ======================================\n",
    "best_models = {\n",
    "    \"Ridge\": gs_ridge.best_estimator_,\n",
    "    \"RandomForest\": gs_rf.best_estimator_,\n",
    "    \"HistGradientBoosting\": gs_hgb.best_estimator_,\n",
    "}\n",
    "\n",
    "valid_scores = {}\n",
    "for name, model in best_models.items():\n",
    "    pred = model.predict(X_valid)\n",
    "    valid_scores[name] = rmse(y_valid, pred)\n",
    "    evaluate_regression(y_valid, pred, title=f\"[VALID] {name}\")\n",
    "    print()\n",
    "\n",
    "pd.Series(valid_scores).sort_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3034c",
   "metadata": {},
   "source": [
    "### Vérification plus robuste (optionnelle)\n",
    "\n",
    "On prend le meilleur modèle selon le hold-out et on estime sa performance avec un **RepeatedKFold** (plus stable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6578c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = min(valid_scores, key=valid_scores.get)\n",
    "best_model = best_models[best_name]\n",
    "print(\"Meilleur modèle (valid RMSE):\", best_name, \"RMSE =\", valid_scores[best_name])\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    best_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=cv_rep,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# On convertit en métriques positives (car neg_* dans sklearn)\n",
    "summary = pd.DataFrame({\n",
    "    \"RMSE\": -cv_results[\"test_neg_rmse\"],\n",
    "    \"MAE\" : -cv_results[\"test_neg_mae\"],\n",
    "    \"R2\"  :  cv_results[\"test_r2\"]\n",
    "})\n",
    "display(summary.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52170110",
   "metadata": {},
   "source": [
    "## 5. Conclusion : évaluation finale + discussion\n",
    "\n",
    "On entraîne le **meilleur modèle** sur `X_train` puis on l’évalue sur `X_valid`.  \n",
    "On discute :\n",
    "- la qualité des prédictions (MAE/RMSE/R²),\n",
    "- l’écart train vs valid (surapprentissage potentiel),\n",
    "- l’intérêt de la transformation `log1p` de la cible pour stabiliser la variance.\n",
    "\n",
    "*(Si c’était une classification on aurait une matrice de confusion ; ici c’est une régression.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Évaluation finale\n",
    "# =========================\n",
    "best_model.fit(X_train, y_train)\n",
    "pred_valid = best_model.predict(X_valid)\n",
    "\n",
    "evaluate_regression(y_valid, pred_valid, title=f\"[FINAL VALID] {best_name}\")\n",
    "\n",
    "# Visualisation: y_true vs y_pred\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_valid, pred_valid, s=8, alpha=0.5)\n",
    "plt.title(\"Validation: y_true vs y_pred\")\n",
    "plt.xlabel(\"Prix réel (lacs)\")\n",
    "plt.ylabel(\"Prix prédit (lacs)\")\n",
    "plt.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Résidus\n",
    "residuals = y_valid - pred_valid\n",
    "plt.figure()\n",
    "plt.hist(residuals, bins=60)\n",
    "plt.title(\"Histogramme des résidus (y_true - y_pred)\")\n",
    "plt.xlabel(\"Résidu (lacs)\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f2262",
   "metadata": {},
   "source": [
    "## (Option) Prédire sur `test.csv` + créer une soumission Kaggle\n",
    "\n",
    "Même si la consigne M2 n’impose pas Kaggle, c’est pratique de produire un fichier final.  \n",
    "On s’aligne sur le format de `sample_submission.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Entraîner sur tout le train et prédire test\n",
    "# ======================================\n",
    "best_model.fit(X_fe, y)\n",
    "test_preds = best_model.predict(test_fe)\n",
    "\n",
    "# Construction submission (le nom de colonne dans sample_submission peut varier)\n",
    "display(sample_sub.head())\n",
    "print(sample_sub.columns)\n",
    "\n",
    "sub = sample_sub.copy()\n",
    "\n",
    "# Cas standard: une colonne cible unique\n",
    "target_like = [c for c in sub.columns if c.lower() != \"id\" and c != sub.columns[0]]\n",
    "# fallback: si sample_sub contient 2 colonnes [ID, TARGET]\n",
    "if len(sub.columns) == 2:\n",
    "    sub.iloc[:, 1] = test_preds\n",
    "else:\n",
    "    # sinon on suppose que la 2ème colonne est la cible\n",
    "    sub.iloc[:, 1] = test_preds\n",
    "\n",
    "SUBMISSION_PATH = \"submission_best_model.csv\"\n",
    "sub.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(\"Submission sauvegardée:\", SUBMISSION_PATH)\n",
    "display(sub.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10235784",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes / pistes d’amélioration (si vous voulez aller plus loin)\n",
    "\n",
    "- Tester des modèles de boosting plus avancés (XGBoost / LightGBM / CatBoost) si autorisés.\n",
    "- Ajouter des features géographiques : interactions LATITUDE/LONGITUDE, clustering de zones, distance à des centres urbains.\n",
    "- Ajouter des transformations sur `SQUARE_FT` ou détecter/traiter d’éventuels outliers.\n",
    "- Feature engineering plus riche sur `ADDRESS` (ex: quartier + ville, hashing, embeddings texte).\n",
    "\n",
    "L’essentiel pour la note : **démarche claire, reproductible, et discussion cohérente** des résultats.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
